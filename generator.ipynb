{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, Model\n",
    "import os\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "raw_data_ds = tf.data.TextLineDataset([\"en_corpus.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\n",
    "for elem in raw_data_ds:\n",
    "   text=text+(elem.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 24\n",
    "step = 3\n",
    "input_chars = []\n",
    "next_char = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - maxlen, step):\n",
    "    input_chars.append(text[i : i + maxlen])\n",
    "    next_char.append(text[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 636413\n",
      "input X  (input_chars)  --->   output y (next_char) \n",
      "He sees me. Charlie drop    --->   p\n",
      "sees me. Charlie dropped    --->    \n",
      "s me. Charlie dropped to    --->    \n",
      "e. Charlie dropped to he    --->   r\n",
      "Charlie dropped to her h    --->   a\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sequences:\", len(input_chars))\n",
    "print(\"input X  (input_chars)  --->   output y (next_char) \")\n",
    "\n",
    "for i in range(5):\n",
    "  print( input_chars[i],\"   --->  \", next_char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ds_raw=tf.data.Dataset.from_tensor_slices(input_chars)\n",
    "y_train_ds_raw=tf.data.Dataset.from_tensor_slices(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase     = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
    "    stripped_punc  =tf.strings.regex_replace(stripped_num, \"[%s]\" % re.escape(string.punctuation), \"\")    \n",
    "    return stripped_punc\n",
    "\n",
    "def char_split(input_data):\n",
    "  return tf.strings.unicode_split(input_data, 'UTF-8')\n",
    "\n",
    "def word_split(input_data):\n",
    "  return tf.strings.split(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 96           # Number of distinct chars / words  \n",
    "embedding_dim = 16             # Embedding layer output dimension\n",
    "sequence_length = maxlen       # Input sequence size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    split=char_split, # word_split or char_split\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(X_train_ds_raw.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary (number of distinct characters):  42\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the vocabulary (number of distinct characters): \", len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tf.squeeze(vectorize_layer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(24,), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(24,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "X_train_ds = X_train_ds_raw.map(vectorize_text)\n",
    "y_train_ds = y_train_ds_raw.map(vectorize_text)\n",
    "\n",
    "X_train_ds.element_spec, y_train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ds=y_train_ds.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (X) dimension:  (64, 24) \n",
      "output (y) dimension:  (64,)\n"
     ]
    }
   ],
   "source": [
    "for sample in train_ds.take(1):\n",
    "  print(\"input (X) dimension: \", sample[0].numpy().shape, \"\\noutput (y) dimension: \",sample[1].numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (sequence of chars):  [ 9  7  3 12  2  5 16  5  8 10  9  4  2  4  7  3  2 17  5 13 13  2  5  0] \n",
      "output (next char to complete the input):  10\n"
     ]
    }
   ],
   "source": [
    "for sample in train_ds.take(1):\n",
    "  print(\"input (sequence of chars): \", sample[0][0].numpy(), \"\\noutput (next char to complete the input): \",sample[1][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence (encoded_sequence):\n",
    "  deceoded_sequence=[]\n",
    "  for token in encoded_sequence:\n",
    "    deceoded_sequence.append(vectorize_layer.get_vocabulary()[token])\n",
    "  sequence= ''.join(deceoded_sequence)\n",
    "  print(\"\\t\",sequence)\n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t farthest console rocked \n",
      "input (sequence of chars):  farthest console rocked  \n",
      "output (next char to complete the input):  b\n"
     ]
    }
   ],
   "source": [
    "for sample in train_ds.take(1):\n",
    "  print(\"input (sequence of chars): \", decode_sequence (sample[0][0].numpy()), \"\\noutput (next char to complete the input): \",vectorize_layer.get_vocabulary()[sample[1][0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "   return np.exp(z)/sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling (conditional_probability, temperature=1.0):\n",
    "  conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n",
    "  conditional_probability = np.log(conditional_probability) / temperature\n",
    "  reweighted_conditional_probability = softmax(conditional_probability)\n",
    "  probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(sequence_length), dtype=\"int64\")\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.LSTM(128, use_bias=False, return_sequences=True)(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.Flatten()(x)\n",
    "predictions=  layers.Dense(max_features, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, predictions,name=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 24, 16)            1536      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 16)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 24, 128)           73728     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1 (Dropout)         (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 24, 128)           131584    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 24, 64)            49408     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                147552    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403808 (1.54 MB)\n",
      "Trainable params: 403808 (1.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9943/9943 [==============================] - 772s 77ms/step - loss: 2.3562 - accuracy: 0.3207\n",
      "Epoch 2/25\n",
      "9943/9943 [==============================] - 919s 92ms/step - loss: 1.7531 - accuracy: 0.4786\n",
      "Epoch 3/25\n",
      "9943/9943 [==============================] - 929s 93ms/step - loss: 1.5929 - accuracy: 0.5217\n",
      "Epoch 4/25\n",
      "9943/9943 [==============================] - 899s 90ms/step - loss: 1.5092 - accuracy: 0.5446\n",
      "Epoch 5/25\n",
      "9943/9943 [==============================] - 867s 87ms/step - loss: 1.4561 - accuracy: 0.5589\n",
      "Epoch 6/25\n",
      "9943/9943 [==============================] - 924s 93ms/step - loss: 1.4195 - accuracy: 0.5689\n",
      "Epoch 7/25\n",
      "9943/9943 [==============================] - 918s 92ms/step - loss: 1.3916 - accuracy: 0.5757\n",
      "Epoch 8/25\n",
      "9943/9943 [==============================] - 936s 94ms/step - loss: 1.3698 - accuracy: 0.5816\n",
      "Epoch 9/25\n",
      "9943/9943 [==============================] - 961s 97ms/step - loss: 1.3520 - accuracy: 0.5869\n",
      "Epoch 10/25\n",
      "9943/9943 [==============================] - 893s 90ms/step - loss: 1.3369 - accuracy: 0.5905\n",
      "Epoch 11/25\n",
      "9943/9943 [==============================] - 965s 97ms/step - loss: 1.3242 - accuracy: 0.5938\n",
      "Epoch 12/25\n",
      "9943/9943 [==============================] - 937s 94ms/step - loss: 1.3139 - accuracy: 0.5966\n",
      "Epoch 13/25\n",
      "9943/9943 [==============================] - 933s 94ms/step - loss: 1.3044 - accuracy: 0.5995\n",
      "Epoch 14/25\n",
      "9943/9943 [==============================] - 982s 99ms/step - loss: 1.2972 - accuracy: 0.6009\n",
      "Epoch 15/25\n",
      "9943/9943 [==============================] - 935s 94ms/step - loss: 1.2895 - accuracy: 0.6032\n",
      "Epoch 16/25\n",
      "9943/9943 [==============================] - 942s 95ms/step - loss: 1.2835 - accuracy: 0.6048\n",
      "Epoch 17/25\n",
      "9943/9943 [==============================] - 946s 95ms/step - loss: 1.2786 - accuracy: 0.6060\n",
      "Epoch 18/25\n",
      "9943/9943 [==============================] - 941s 95ms/step - loss: 1.2734 - accuracy: 0.6072\n",
      "Epoch 19/25\n",
      "9943/9943 [==============================] - 932s 94ms/step - loss: 1.2684 - accuracy: 0.6087\n",
      "Epoch 20/25\n",
      "9943/9943 [==============================] - 919s 92ms/step - loss: 1.2650 - accuracy: 0.6099\n",
      "Epoch 21/25\n",
      "9943/9943 [==============================] - 940s 95ms/step - loss: 1.2617 - accuracy: 0.6106\n",
      "Epoch 22/25\n",
      "9943/9943 [==============================] - 927s 93ms/step - loss: 1.2580 - accuracy: 0.6110\n",
      "Epoch 23/25\n",
      "9943/9943 [==============================] - 925s 93ms/step - loss: 1.2543 - accuracy: 0.6124\n",
      "Epoch 24/25\n",
      "9943/9943 [==============================] - 937s 94ms/step - loss: 1.2510 - accuracy: 0.6132\n",
      "Epoch 25/25\n",
      "9943/9943 [==============================] - 919s 92ms/step - loss: 1.2479 - accuracy: 0.6137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d493e1b390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_original, step,temperatures=[]):\n",
    "    seed= vectorize_text(seed_original)\n",
    "    print(\"The prompt is:\",end='')\n",
    "    decode_sequence(seed.numpy().squeeze())\n",
    "    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
    "    #Text Generated by Temperature Sampling\n",
    "    print(\"Text Generated by Temperature Sampling:\")\n",
    "    for temperature in temperatures:\n",
    "        print(\"\\ttemperature: \", temperature)\n",
    "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
    "        generated_temperature = (seed)\n",
    "        for i in range(step):\n",
    "            predictions=model.predict(seed)\n",
    "            next_index = temperature_sampling(predictions.squeeze(), temperature)\n",
    "            generated_temperature = np.append(generated_temperature, next_index)\n",
    "            seed= generated_temperature[-sequence_length:].reshape(1,sequence_length)\n",
    "        print(\"Output :\",end=\"\")\n",
    "        decode_sequence(generated_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is:\t charlie \n",
      "Text Generated by Temperature Sampling:\n",
      "\ttemperature:  0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 787ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Output :\t charlie xxjjjbpllovvvv    voaav’vybccarcov’kx just in the party from the door the side of\n"
     ]
    }
   ],
   "source": [
    "generate_text(model,\"charlie \",100,[0.2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
